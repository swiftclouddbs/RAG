#https://python.langchain.com/v0.2/docs/tutorials/rag/


from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import PromptTemplate
from langchain import hub
from langchain_chroma import Chroma
#from langchain_community.document_loaders import WebBaseLoader
from langchain_community.document_loaders import PyPDFLoader
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
#from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings import SentenceTransformerEmbeddings
#from langchain_nomic.embeddings import NomicEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Load, chunk and index the contents of the blog.
# loader = WebBaseLoader(
#     web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
#     bs_kwargs=dict(
#         parse_only=bs4.SoupStrainer(
#             class_=("post-content", "post-title", "post-header")
#         )
#     ),
# )

#Set llm
local_llm = "llama3.1"
llm = ChatOllama(model=local_llm, format="json", temperature=0)

#ID directory to persist data
#client = chromadb.PersistentClient(path="/Users/reginaldstuart/Documents/IT/ML:AI/myRAG/chroma_db")

#create chroma_client and connect to server.  
#chroma_client = chromadb.HttpClient(host='localhost', port=8000)

#Select files we want to input
files = [
    "America.pdf",
]

#Load files
docs = [PyPDFLoader(file).load() for file in files]
docs_list = [item for sublist in docs for item in sublist]

#Split files
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=1000, chunk_overlap=100
)


#Here is the data we want to load
doc_splits = text_splitter.split_documents(docs_list)

#vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())
vectorstore = Chroma.from_documents(
    documents=doc_splits,
#    collection_name="new_col_4",
#    embedding=NomicEmbeddings(model="nomic-embed-text-v1.5", inference_mode="local"),
    embedding=SentenceTransformerEmbeddings(),
#    persist_directory="./new_db_4",
)


# Retrieve and generate using the relevant snippets of the blog.
retriever = vectorstore.as_retriever()
#prompt = hub.pull("rlm/rag-prompt")
prompt = PromptTemplate(
    template="""<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks. 
    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. 
    Use five sentences maximum <|eot_id|><|start_header_id|>user<|end_header_id|>
    Question: {question} 
    Context: {context} 
    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>""",
    input_variables=["question", "context"],
)

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# Chain
rag_chain = prompt | llm | StrOutputParser()

# Run
question = "What year was the map of America made?"
docs = retriever.invoke(question)
generation = rag_chain.invoke({"context": docs, "question": question})
print(generation)

    
#https://python.langchain.com/v0.2/docs/tutorials/rag/
# rag_chain = (
#     {"context": retriever | format_docs, "question": RunnablePassthrough()}
#     | prompt
#     | llm
#     | StrOutputParser()
# )

# rag_chain.invoke("When was the map made?")
