#Adapted from vendor code:  https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_rag_agent_llama3_local.ipynb
# Persistance help:  https://stackoverflow.com/questions/76232375/langchain-chroma-load-data-from-vector-database

## Loads text docs into Chroma vector store
## Catalog of document loaders is here:  https://python.langchain.com/v0.2/docs/integrations/document_loaders/

from langchain_community.document_loaders import PyPDFLoader
#from langchain_community.document_loaders import TextLoader
from chromadb.utils import embedding_functions
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_nomic.embeddings import NomicEmbeddings
#import chromadb
#from sentence_transformers import SentenceTransformer
from langchain_community.embeddings import SentenceTransformerEmbeddings
import gpt4all

#Set llm
local_llm = "llama3.1"

#ID directory to persist data
#client = chromadb.PersistentClient(path="/Users/reginaldstuart/Documents/IT/ML:AI/myRAG/chroma_db")

#create chroma_client and connect to server.  
#chroma_client = chromadb.HttpClient(host='localhost', port=8000)

#Select files we want to input
files = [
    "Flight_Training_Guide.pdf",
]

#Load files
docs = [PyPDFLoader(file).load() for file in files]
docs_list = [item for sublist in docs for item in sublist]

#Split files
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=1000, chunk_overlap=100
)


#Here is the data we want to load
doc_splits = text_splitter.split_documents(docs_list)



# Add to vectorDB
vectorstore = Chroma.from_documents(
    documents=doc_splits,
    collection_name="test_col_1",
    embedding=NomicEmbeddings(model="nomic-embed-text-v1.5", inference_mode="local"),
#    embedding=embedding_functions.DefaultEmbeddingFunction(),
#    embedding=SentenceTransformerEmbeddings(),
    persist_directory="./ccbc_rag_db_1",
)

retriever = vectorstore.as_retriever()

##Now you should be able to read the vectorstore from another window

XXXXXXXX
### Generate

from langchain_community.chat_models import ChatOllama
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate

# Prompt
prompt = PromptTemplate(
    template="""<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks. 
    Scan the document to answer the question.  Use ten sentences maximum <|eot_id|><|start_header_id|>user<|end_header_id|>
    Question: {question} 
    Context: {context} 
    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>""",
    input_variables=["question", "document"],
)

# prompt = PromptTemplate(
#     template="""<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks. 
#     Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. 
#     Use three sentences maximum and keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>
#     Question: {question} 
#     Context: {context} 
#     Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>""",
#     input_variables=["question", "document"],
# )

llm = ChatOllama(model=local_llm, temperature=0)


# Post-processing
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


# Chain
rag_chain = prompt | llm | StrOutputParser()

# Run
question = "What license is needed to fly for major airlines?"
docs = retriever.invoke(question)
generation = rag_chain.invoke({"context": docs, "question": question})
print(generation)


